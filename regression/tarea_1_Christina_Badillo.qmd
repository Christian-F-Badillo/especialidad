---
title: "Regresión"
subtitle: "Tarea 1"
author: 
    - name: "Christian Badillo"
format:
    pdf:
        toc: true
        documentclass:  article
        number-sections: true
        colorlinks: true
        highlight-style: github
        fontfamily: times
        fontsize: 12pt
        geometry: "left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm"
        lof: false
        lot: false
        code-block-bg: true
        code-block-border-left: "#31BAE9"
        pdf-engine: pdflatex
        bibliography: ref.bib
        csl: apa
nocite: |
  @*
crossref:
  fig-prefix: figura   # (default is "Figure")
  tbl-prefix: tabla    # (default is "Table")
editor: source
lang: es-ES
execute:
    cache: true
    freeze: auto 
---

\newpage

# Ejercicio 0 (clase).

Encuentra los Estimadores de Mínimos Cuadrados de los parámetros $\beta_0$ y $\beta_1$ minimizando la función:

$$
f\left( \beta_0, \beta_1 \right) = \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 x_i \right) ^2
$$

## Solución.

Derivamos la función $f\left( \beta_0, \beta_1 \right)$ con respecto a $\beta_0$ y $\beta_1$ e igualamos a cero:
\begin{align}
\frac{\partial f}{\partial \beta_0} &= -2 \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 x_i \right) = 0 \label{eq:1} \\
\frac{\partial f}{\partial \beta_1} &= -2 \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 x_i \right) x_i = 0 \label{eq:2}
\end{align}

Se divide entre $-2$ y se distribuyen las sumatorias en \ref{eq:1}:
\begin{align*}
\sum_{i=1}^{n} y_i - \sum_{i=1}^{n} \beta_0 - \sum_{i=1}^{n} \beta_1 x_i &= 0 \\
n \bar{y} - n \beta_0 - \beta_1 n \bar{x} &= 0 && \text{Utilizando } \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i \text{ y } \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i \\
\bar{y} - \beta_0 - \beta_1 \bar{x} &= 0 && \text{Factorizando } n
\end{align*}

Realizamos el mismo procedimiento con \ref{eq:2}:
\begin{align*}
\sum_{i=1}^{n} y_i x_i - \sum_{i=1}^{n} \beta_0 x_i - \sum_{i=1}^{n} \beta_1 x_i^2 &= 0 && \text{Distribuyendo la sumatoria y } x_i \\
\sum_{i=1}^{n} y_i x_i - n \beta_0 \bar{x} - \beta_1 \sum_{i=1}^{n} x_i^2 &= 0 && \text{Utilizando } \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i \\
\end{align*}

Obtenemos el sistema de ecuaciones:
\begin{align}
\bar{y} - \beta_0 - \beta_1 \bar{x} &= 0 \label{eq:3} \\
\sum_{i=1}^{n} y_i x_i - n \beta_0 \bar{x} - \beta_1 \sum_{i=1}^{n} x_i^2 &= 0 \label{eq:4}
\end{align}

Despejamos $\beta_0$ de \ref{eq:3}:
\begin{align*}
\beta_0 &= \bar{y} - \beta_1 \bar{x} \\
\end{align*}

Sustituimos $\beta_0$ en \ref{eq:4} y resolvemos para $\beta_1$:
\begin{align*}
\sum_{i=1}^{n} y_i x_i - n \left( \bar{y} - \beta_1 \bar{x} \right) \bar{x} - \beta_1 \sum_{i=1}^{n} x_i^2 &= 0 \\
\sum_{i=1}^{n} y_i x_i - n \bar{y} \bar{x} + n \beta_1 \bar{x}^2 - \beta_1 \sum_{i=1}^{n} x_i^2 &= 0 && \text{Distribuyendo } n \\
n \beta_1 \bar{x}^2 - \beta_1 \sum_{i=1}^{n} x_i^2 &= - \sum_{i=1}^{n} y_i x_i + n \bar{y} \bar{x} \\
- \beta_1 \left( \sum_{i=1}^{n} x_i^2 - n \bar{x}^2 \right) &= - \sum_{i=1}^{n} y_i x_i + n \bar{y} \bar{x} && \text{Factorizando } \beta_1 \\
- \beta_1 s^2_X &= n \bar{x} \bar{y} - \sum_{i=1}^{n} y_i x_i && \text{Se usa la relación } s^2_X = \sum_{i=1}^{n} x_i^2 - n \bar{x}^2 \\
\beta_1 s^2_X &= s_{XY} && \text{Donde } s_{XY} = n \bar{x} \bar{y} - \sum_{i=1}^{n} y_i x_i \\
\beta_1 &= \frac{s_{XY}}{s^2_X} 
\end{align*}

Por último, sustituimos $\beta_1$ en \ref{eq:3} y despejamos $\beta_0$:
\begin{align*}
\beta_0 &= \bar{y} - \frac{s_{XY}}{s^2_X} \bar{x} \\
\end{align*}

Por lo tanto, los estimadores de mínimos cuadrados de los parámetros $\beta_0$ y $\beta_1$ son:
\begin{align*}
\hat{\beta_0} &= \bar{y} - \frac{s_{XY}}{s^2_X} \bar{x} \\
\hat{\beta_1} &= \frac{s_{XY}}{s^2_X}
\end{align*}

# Ejercicio 2.

1. Menciona los supuestos sobre los errores $\epsilon_i$ para el modelo de regresiónn lineal simple.

    1. Su valor esperado es 0, $\mathbb{E}(\epsilon_i) = 0$.
    2. Homocedasticidad, es decir, la varianza de los errores es constante, $Var(\epsilon_i) = \sigma^2$.
    3. Los errores son no correlacionados entre sí, $Cov(\epsilon_i, \epsilon_j) = 0, i \neq j$.
    
2. ¿Qué le da el carácter aleatorio al modelo de regresión lineal simple?

EL término $\epsilon$ del modelo dado que nos permite añadir un termino estocástico a un proceso determinístico como lo es la función del modelo (en este caso una función lineal).

3. ¿Cuáles de los siguientes pueden considerarse modelos de regresión lineal? ¿Por qué?
    
    * $f(\beta_0, \beta_1) = \beta_0 + e^{\beta_1 x_1}$. 

    No es un modelo lineal dado que para tiene una no linealidad sobre uno de sus parámetros en este caso $\beta_1$ y no es posible eliminarla.

    * $f(\beta_0, \beta_1, \dots, \beta_k) = \beta_0 + \sum^{k}_{i=1} \beta_i \cosh^i{x_i}$.

    Es un modelo lineal dado que es una combinación lineal de los parámetros.

    * $f(\beta_0, \beta_1) = \beta_0 + \beta_1 x_1 + \beta_0 \beta_1 x_2$.

    No es un modelo lineal dado que hay dos parámetros que interactuán de manera multiplicativa entre ellos.
    
# Ejercicio 3.

Supongamos que la variable aleatoria $\epsilon$ sigue una distribución normal con media $\mu$ y varianza $\sigma^2$, y que, para $1 \leq i \leq n$, la variables aleatorias $\epsilon_i$ son independientes y cada una sigue una distribución normal con media $\mu_i$ y varianza $\sigma_i$.

* ¿Cómo podemos asociar $\epsilon$ a una variable aleatoria que sigue una distribución normal estándar?

La podemos asociar por medio de una combinación lineal, ya que la combinación lineal de variables aleatorias normales independientes sigue una distribución normal.

* ¿Qué distribución siguen las siguientes variables aleatorias?

1. Para $a, b \in \mathbb{R}$, con $a \neq 0, X = a\epsilon + b$.

La variable aleatoria X sigue una distribución normal con media $a\mu_\epsilon + b$ y varianza $(a\sigma_\epsilon )^2$.

***Demostración.***

Teorema 2.1.5 en Casella y Berger (2002):

Si X es una variable aleatoria con pdf, $f_X(x)$, continua sobre su soporte $\mathcal{X}$ y $Y = g(X)$ con soporte $\mathcal{Y}$ y con $g$ una función monótona. Y suponiendo que $g^{-1}(y)$ es diferenciable en  $\mathcal{Y}$. Entonces:
\begin{equation*}
f_Y(y) = \frac{d}{dy} F_Y(y) = 
\begin{cases}
f_X \left( g^{-1}(y) \right) \left| \frac{d}{dy} g^{-1}(y) \right|, & y \in \mathcal{Y} \\
0, & \text{en otro caso}.
\end{cases}
\end{equation*}

Usando los datos del problema:
$$
f_X(x) = f_\epsilon (\frac{x - b}{a}) \left|\frac{d}{dx} \frac{x - b}{a} \right| = \frac{1}{\sqrt{2\pi \sigma_\epsilon^2}}e^{-\frac{ \left( \frac{x - b}{a} - \mu_\epsilon \right)^2}{2\sigma_\epsilon^2}} \frac{1}{|a|} 
$$
Con un poco de algebra:
$$
f_X(x) = \frac{1}{\sqrt{2\pi \sigma_\epsilon^2 a^2}} e^{-\frac{(x - (a\mu_\epsilon + b))^2}{2\sigma_\epsilon^2a^2}}
$$

Con lo que llegamos a la conclusión de que la variable aleatoria X sigue una distribución normal con media $a\mu_\epsilon + b$ y varianza $(a\sigma_\epsilon )^2$.

2. $X = \epsilon_1 + \epsilon_2$.

La variable aleatoria X sigue una distribución normal con media $\mu_{\epsilon_1} +\mu_{\epsilon_2}$ y varianza $\sigma_{\epsilon_1}^2 + \sigma_{\epsilon_2}^2$.

***Demostración.***

Dada la función generadora de momentos de la distribución normal:
$$
M_\epsilon (t) = e^{t\mu + \frac{t^2\sigma^2}{2}}.
$$

Y que la función generadora de momentos de una combinación lineal de variables aleatorias independientes es el producto de sus funciones generadora de momentos individuales. Entonces:
\begin{align*}
M_{\epsilon_1 + \epsilon_2} (t) &= \mathbb{E} \left( e^{(\epsilon_1 + \epsilon_2)t}\right) \\
&= M_{\epsilon_1}(t) M_{\epsilon_2} (t) \\ 
&= e^{t\mu_{\epsilon_1} + \frac{t^2\sigma_{\epsilon_1}^2}{2}} e^{t\mu_{\epsilon_2} + \frac{t^2\sigma_{\epsilon_2}^2}{2}} \\
&= e^{t (\mu_{\epsilon_1} + \mu_{\epsilon_1}) + \frac{t^2 (\sigma_{\epsilon_1}^2 + \sigma_{\epsilon_2}^2)}{2}}
\end{align*}

Lo cual nos da la función generadora de momentos de la distribución ya mencionada.

3. $X = \epsilon_1 - \epsilon_2.$

La variable aleatoria X sigue una distribución normal con media $\mu_{\epsilon_1} - \mu_{\epsilon_2}$ y varianza $\sigma_{\epsilon_1}^2 + \sigma_{\epsilon_2}^2$.

***Demostración.***

Usando los hechos anteriormente demostrados podemos usar sus resultados para obtener que la distribución de $Y = -\epsilon_i$ es una distribución normal con media $-\mu_{\epsilon_i}$ (se utiliza $a = - 1 \text{ y } b =0$) y varianza $(-1)^2\sigma_{\epsilon_i}^2 = \sigma_{\epsilon_i}^2$. Usando este resultado intermedio aplicado al resultado demostrado en el inciso anterior se puede concluir que la distribución de la variable aleatoria X es una distribución normal con media $\mu_{\epsilon_1} - \mu_{\epsilon_2}$ y varianza $\sigma_{\epsilon_1} + \sigma_{\epsilon_2}$.

4. $X = \epsilon_1 \epsilon_2$.

En la siguiente entrada de Wikipedia ["Distribution of the product of two random variables"](https://en.wikipedia.org/wiki/Distribution_of_the_product_of_two_random_variables#Independent_central-normal_distributions) en el apartado de "Independent central-normal distributions" se muestra que el resultado que sigue el producto de dos distribuciones normales estándar es una distribución no normal. En particular X sigue la siguiente distribución:

$$
P_X(x) = \frac{1}{\pi} K_0(|x|), \text{ donde } K_0 \text{ es la función de Bessel modificada de segundo tipo.}
$$

Y su soporte es $(-\infty, \infty)$. Y cuya obtención va más allá de los conocimientos de este alumno.

5. $X = \frac{\epsilon_1}{\epsilon_2}$.

Usando una generalización del teorema 2.1.5 de Casella y Berger (2002) se puede calcular la distribución de la variable aleatoria X. Dado que $\epsilon_1$ y $\epsilon_2$ son variables aleatorias normales independientes, se puede demostrar que la variable aleatoria X sigue una distribución Cauchy estándar, como lo muestran en su ejemplo 4.3.6.

6. $X = \frac{\epsilon^2}{\sigma^2}$.

Usando el teorema 2.1.5 de Casella y Berger (2002) se puede calcular la distribución de la variable aleatoria X. Dado que $\epsilon^2$ es una distribución $\chi^2$ con un grado de libertad y $\sigma^2$ es una constante, usemos el teorema para calcular la distribución de X.

$$
f_X(x) = f_{\epsilon^2} \left( x\sigma^2 \right) \left| \frac{d}{dx} x\sigma^2 \right| = \frac{1}{2^{\frac{1}{2}}\Gamma(\frac{1}{2})} (x\sigma^2)^{-\frac{1}{2}} e^{-\frac{x\sigma^2}{2}} \sigma^2
$$

Simplificando:

$$
f_X(x) = \frac{\sigma}{2^{\frac{1}{2}}\Gamma(\frac{1}{2})} x^{-\frac{1}{2}} e^{-\frac{x\sigma^2}{2}} = Gamma(\frac{1}{2}, \sigma^2)
$$

Por lo tanto, la variable aleatoria X sigue una distribución Gamma con parámetros $\frac{1}{2}$ y $\sigma^2$.

1. $X = \sqrt{\epsilon_1^2 + \epsilon_2^2}$.

Usando la definición de la distribución $\chi$:

$$
Y_k = \sqrt{\sum_{i=1}^k Z_i^2}, \hspace{0.5cm} Z_i \sim N(0,1).
$$

Se observa que X define a una distribución $\chi$ con dos grados de libertad.


8. $X = \sum_{i=1}^n \epsilon_i^2$.

Dado que la distribución $\chi^2$ de un grado de libertad se define como el cuadrado de una variable aleatoria normal estándar, se puede concluir que cada $\epsilon_i^2$ se distribuye como una $\chi^2$ con un grado de libertad. Además una de las propiedades de la distribución $\chi^2$ es que la suma de $n$ variables aleatorias $\chi^2$ con un grado de libertad es una variable aleatoria $\chi^2$ con $n$ grados de libertad. Por lo tanto, la variable aleatoria X se distribuye como una $\chi^2$ con $n$ grados de libertad. 

9.  $X = \frac{\frac{\sum_{i=1}^n \epsilon^2}{n}}{\frac{\sum_{i=1}^m \eta^2}{m}}$

Usando el resultado anterior, se observa que la distribución de las sumatorias es una $\chi^2$ con $n$ y $m$ grados de libertad respectivamente. Y dado que la distribución F Fisher-Snedecor se define como el cociente de dos variables aleatorias $\chi^2$ divididas por sus grados de libertad, se puede concluir que la variable aleatoria X sigue una distribución F con $n$ y $m$ grados de libertad.

\newpage
# Ejercicio 3.

La siguiente es una tabla que nos da las estaturas(en centímetros) y pesos(en kilogramos) de una muestra de 10 mujeres adolescentes de 18 años. Se busca intentar predecir el peso de acuerdo a la altura.

```{r}
#| echo: false
#| tbl-cap: Datos

library(kableExtra)
library(knitr)

altura <- c(169.6, 166.8, 157.1, 181.1, 158.4, 165.6, 166.7, 156.5, 168.1, 165.3)
peso <- c(71.2, 58.2, 56.0, 64.5, 53.0, 52.4, 56.8, 49.2, 55.6, 77.8)

df <- data.frame(Altura = altura,
                 Peso = peso)

kable(df)
```

Gráfica la altura contra el peso, ¿dirías que tiene sentido utilizar un modelo de regresión lineal para estos datos? En caso afirmativo, aproxima tal modelo con lo obtenido en el primer ejercicio, y luego dibuja tal aproximación sobre la gráfica previamente obtenida. ¿Consideras que el modelo es bueno? ¿Por qué?

```{r}
#| echo: false
#| fig-cap: Gráfico de dispersión de los datos.
#| warning: false
library(tidyverse)
library(tidyplots)
my_tible <- tibble(Altura = altura,
                   Peso = peso)
# Scatter plot
my_tible %>%
  ggplot(aes(x = Altura, y = Peso)) +
  geom_point() +
  labs(title = "",
       x = "Altura (cm)",
       y = "Peso (kg)")
```

No parece que la relación entre el peso y la altura sea linealmente "fuerte", dado que no se observa una tendencia clara en los datos. De hecho parece que la relación es más bien cuadrática. Por lo tanto, un modelo de regresión lineal no parece adecuado para estos datos.

```{r}
#| echo: false
cov_data <- cov(altura, peso)
var_data <- var(altura)

beta_1 <- cov_data / var_data
beta_0 <- mean(peso) - beta_1 * mean(altura)
```

El modelo de regresión lineal ajustado es: Peso = `r round(beta_0, 2)` + `r round(beta_1, 2)` * Altura.

```{r}
#| echo: false
#| fig-cap: Ajuste del Modelo de Regresión Lineal Simple.
#| label: fig:regresion
#| warning: false
my_tible <- tibble(Altura = altura,
                   Peso = peso)
# Scatter plot
my_tible %>%
  ggplot(aes(x = Altura, y = Peso)) +
  geom_abline(intercept = beta_0, slope = beta_1, color = "red") +
  geom_point() +
  labs(title = "",
       x = "Altura (cm)",
       y = "Peso (kg)")
```

El ajuste del modelo es malo ya que se observan muchos puntos alejados de la recta de regresión, lo que se traduce que los errores de predicción son grandes para los datos observados y probablemente para datos no observados, por lo que se concluye que el modelo no es bueno para modelar la relación entre la altura y el peso.

\newpage
# Referencias